​																	从“回”字的四种写法到变体文本识别

>  有一回对我说道，“你读过书么？……我便考你一考。茴香豆的茴字，怎样写的？”......“回字有四样写法，你知道么？”....孔乙己刚用指甲蘸了酒，想在柜上写字，见我毫不热心，便又叹一口气，显出极惋惜的样子。

<img src="http://tva3.sinaimg.cn/large/e16fc503gy1fmpx9uolfvj20du0fm3yw.jpg" alt="对,我说过一 - 冬至要吃饺子——鲁迅_鲁迅表情" style="zoom:33%;" />

最近工作中涉及文本数据处理相关的内容，想起《孔乙己》中关于“回”字有四种写法的片段，原文这部分描写刻画了孔乙己的迂腐形象，我这里以此为引想分享的是关于变体文本识别的简单思路。

文中提到的“回字有四样写法”，这里“回”是指”茴“字去掉草字头后下半部分的”回“，而这四种写法分别是”回“、”囘“、”囬“、“hui“(第4个我打不出来)，典出《康熙字典》。

![img](https://t11.baidu.com/it/u=1543822963,2537990798&fm=173&app=49&f=JPEG?w=400&h=71&s=0BAE7A224B6347245CDD04DF0000A0B1)

首先简单观察归纳一下这四种写法有以下特征

1. 首先四字的拼音相同，均为“hui”

2. 除第2字”囘“外，其余3字中均有“口”字部首/部分

3. “回”字笔画数为6，”囘“为5、”囬“为7，4th为8。同音形近字笔画接近

另外结合我们平时的阅读经验，如果字里行间偶尔出现一两个生僻字或者错别字，大多数时候并不影响我们获取这部分文字携带的信息，例如比较朴素的方法是“认字认一半”，而且如果我们对文字内容的背景或主题越熟悉，那么对文本中出现生僻/错字的容纳程度也越高(即生僻/错字多一点也不影响阅读)，结合上下文我们能够自动纠错。例如：

> 1. 我给他打电话里，他说马上就囬来了。
> 2. 等他返囘我就让他去接你

这四种"hui"字在大部分情况下使用任一个都不影响我们阅读，但是我们日常使用一般只会用到“回”，而不会使用其他三种，并将其他三种当作生僻字或者错别字。在此基础上扩展，本文将这种**不影响目标用户获取文本信息**的生僻字/错别字/同音字等称为变体文本，变体文本一般有如下特征：

1. 同音字；如 微信 > 威信/薇信
2. 字形相近，偏旁部首有交集或笔画数接近；如 手机号  > 手几号

我们再来看一个例子：

> 感兴趣就邦定手虮号，加威信XXXXXX

经常网上冲浪的同学可能看到过上述类似的引流信息，最开始的黑产引流大多数使用普通文本引流，各平台逐步接入风控之后，使用文本匹配规则策略可以屏蔽大部分引流文本，但黑产的对抗方式是使用变体“加密”文本，这种低成本的手段在某种程度可以绕过大部分基于匹配规则的文本风控策略，而且不影响黑产对目标用户传达有效引流信息。

#### 风控对抗

变体文本出现在各种场景，黑产主要用其引流，而非黑产则是在社交网络/论坛等使用变体文本发布一些敏感/违规内容，例如涉政等。而安全风控的目标是**识别出变体文本**，尽可能还原其真实(目标)文本并对文本进行处理&分析判断。目前对于变体文本识别，目前主流方法有基于关键词匹配、基于词向量预测(变体词分词效果不理想)、基于深度学习识别。我这里主要介绍一种借鉴关键词匹配思路的简单且朴素的识别方法。

#### 主要分以下三步骤

1. 基于业务场景维护风险词库
2. 对文本进行转换处理
3. 相似度评估

为方便介绍，以下用脱敏数据示例过程

##### 1. 维护风险词库 

>  如在人不符场景的风险词有：换人、微信、电话、手机号等

##### 2. 文本转换处理

基于上述对“hui”字的分析和扩展，文本处理转换主要可进行以下三种：

* 文字转拼音
* 文字拆分
* 文字笔画统计

以一条黑产引流文本为例说明过程

> 噂儆的碦戸：其鎃祝册手茺贈宋888葒笣！（还原文本：尊敬的客户:棋牌注册首充赠送888红包!）

###### 2.1 拼音转换

```python
from pypinyin import pinyin, Style
#pypinyin韵律识别模块  https://pypi.org/project/pypinyin/ 
def cn_toPinyin(text):
  #文本转拼音，res为带声调结果，例['zun3'] zun-3声；heteronym为多音字输出参数
    res = pinyin(text, style=Style.TONE3, heteronym=False)
    simp_res = [i[0][0:-1] for i in res]
    return simp_res

cn_toPinyin('噂儆锝碦戸其鎃祝册手茺贈宋888葒笣')
```

cn_toPinyin()转拼音结果：

不带声调['zun','jing','de','ke','hu','qi','pai','zhu','ce','shou','chong','zeng','song','88','hong','bao']

带声调['zun3','jing3','d2','ke4','hu4','qi2','pai4','zhu4','ce4','shou3','chong1','zeng4','song4','88','hong2','bao1']

###### 2.2 文字拆分

主要是拆分汉字的**偏旁和主体字**，这部分工作有两种思路，第一种是维护本地的拆字字典，第二种通过在线接口获取

* 本地拆字字典数据样例如下，我们主要目的是获取其主体字

![截屏2022-06-18 下午5.00.02](/Users/didi/Library/Application Support/typora-user-images/截屏2022-06-18 下午5.00.02.png)

``` python
import os
# 读取本地字典
path = './chaizi-jt.txt'
dic = open(path, encoding='UTF-8')
for l in dic:
    l = l.strip()
    k, v = l.split('\t')[0:2]
    kv[k] = v

def split_cn(text):
    ori_t = [i for i in text]
#     拆字结果
    bs_t = [kv[h] if h in kv.keys() else h for h in ori_t]
#     保留主体字
    main_zn = [kv[h][2:] if h in kv.keys() else h for h in ori_t]
    
    return main_zn

ori = '噂儆锝碦戸其鎃祝册手茺贈宋葒笣'
#转换之前剔除标点符号和数字
print(split_cn(ori)) 
```

split_cn()转换结果

拆字：['口 尊', '人 敬', '金 日 一 寸', '石 客', '一 尸', '甘 一 八', '鎃', '示 兄', '⺆ ⺆ 一', '丿 二 亅', '草 充', '贈', '宀 木', '葒', '竹 包']

仅保留主体字：['尊', '敬', '日 一 寸', '客', '尸', '一 八', '鎃', '兄', '⺆ 一', '二 亅', '充', '贈', '木', '葒', '包']

* 在线接口可爬[百度汉语](https://hanyu.baidu.com/zici/)或者[在线新华字典](http://xh.5156edu.com/)，这部分脚本可参考[汉字部首获取](https://github.com/WenDesi/Chinese_radical)。这里存在一个问题是这两个接口只能获取准确的部首，无法获取主体字；我这里提供个人思路供参考，以百度汉语为例，取部首处的元素可获取部首，主体字可获取“相关字”标签下的首字，准确度不高，另外可叠加拼音辅助判断；

* <img src="/Users/didi/Desktop/截屏2022-06-18 下午5.06.50.png" alt="截屏2022-06-18 下午5.06.50" style="zoom:50%;" />

  

###### 2.3 笔画统计

* 笔画统计部分实现同样可分本地库和在线接口获取，[百度汉语](https://hanyu.baidu.com/zici/)有可解析的“笔画”数据，不再赘述

  本地拆字字典数据样例如下

  ![截屏2022-06-18 下午6.13.07](/Users/didi/Library/Application Support/typora-user-images/截屏2022-06-18 下午6.13.07.png)

  ```python
  path = './sin_chinese_feature.txt'
  dic = open(path, encoding='UTF-8')
  for l in dic:
      l = l.strip()
      k, v = l.split(' ')[0:2]
      kv[k] = v
  
  def cn_bihua(text):
      ori_t = [i for i in text]
      bs_t = [len(kv[h]) if h in kv.keys() else h for h in ori_t]
      return bs_t
    
  ori = '噂儆锝碦戸其鎃祝册手茺贈宋葒笣'
  print(cn_bihua(ori))
  ```

  cn_bihua()转换结果：[15, 14, 13, 14, 4, 8, 17, 9, 5, 4, 9, 19, 7, 12, 11]

##### 3. 相似度评估

我们这里做的是基于关键词匹配的思路，即评估文本中是否有风险词库中字词的变体，以上述样本为例，对应风控场景的风险词库一般包含“棋牌、注册，首充，红包”等词，需要判断文本中实际是否包含这些词。

将上述的文本转换处理封装之后，对于一条短文本，转换为key-value数据，key为原字，value为三种转换结果，例如

```python
{
    "ori":"茺",
    "handled":{
        "pinyin":"'chong",
        "zhuti":"充",
        "bihua":"9"
    }
}
```

相似分度量，这里是demo测试，打分比较粗糙，拼音和拆字特征取了最高分，因评估词维度，故词打分取了字的最低分；另，在测试过程中弃用了笔画数特征，误伤较多，该特征和拆字特征结合打分可能更合适

```python
def text_handle(ori_text):
  #字维度的处理，转拼音，拆字，统计笔画数
    done = {}
    simp_res = cn_toPinyin(ori_text)
    main_zn = split_cn(ori_text)
    bs_t = cn_bihua(ori_text)
    done["ori"] = ori_text
    done["handled"] = dict({"pinyin":simp_res[0], "zhuti":main_zn[0], "bs_t":bs_t[0]})
    return done
  
def v_compare(risk_vac, slide_v, c_len):
    c_p =0
    v_score = {}
    while c_p < c_len:
        if slide_v[c_p] == risk_vac[c_p]:
            sim_score = 1 
            v_score[slide_v[c_p]] = sim_score
        else:
            ori = slide_v[c_p]
            handled = text_handle(slide_v[c_p])['handled']
            py = handled['pinyin'] #拼音
            zhuti = handled['zhuti'] #拆字，评估这里不取主体字，直接用拆字的完整结果
            bs_t = handled['bs_t']

            risk = risk_vac[c_p] 
            risk_handled = text_handle(risk_vac[c_p])['handled']
            r_py = risk_handled['pinyin'] #拼音
            r_zhuti = risk_handled['zhuti'] #拆字，评估这里不取主体字，直接用拆字的完整结果
            r_bs_t = risk_handled['bs_t'] 
    #          分数
            py_sim_socre = len(set(py)&set(r_py))/len(set(py)|set(r_py))
            cz_sim_socre = len(set(zhuti)&set(r_zhuti))/len(set(zhuti)|set(r_zhuti))
            #测试数据显示笔画数不适合用做打分
#             bh_sim_socre = min(bs_t, r_bs_t)/max(bs_t, r_bs_t) 
#             sim_score = max(py_sim_socre, cz_sim_socre, bh_sim_socre)
            sim_score = max(py_sim_socre, cz_sim_socre)
            v_score[slide_v[c_p]] = sim_score
        c_p += 1
#     词维度相似分，取字维度最低分
    v_score = min(v_score.values())
    return v_score
  
def risk_v():
    path = './risk_vac.txt'
    dic = open(path, encoding='UTF-8')
#     风险词的词字数len(i.strip())
    risk_v = [(i.strip(), len(i.strip())) for i in dic]
    return risk_v

if __name__ == "__main__":
    loc_risk = risk_v()
    #为测试>2个字的风险词，额外构造新样本
    ori_text = '噂儆锝碦戸其鎃祝册手茺贈宋葒笣筷垫莱瓶治'
    print("目标文本:",ori_text)
    for v in loc_risk:
        vac = v[0] 
        vac_cnt = v[1]
        ori_len = len(ori_text) #原始文本长度
        p = 0
        while p < (ori_len-vac_cnt+1):
    #         滑动窗口取词
            cur_vac = ori_text[p: p+vac_cnt]
            score = v_compare(vac, cur_vac, vac_cnt)
            p += 1
            if score > 0.3:
                print("风险词:",vac, "目标词:",cur_vac, "相似分:",score)
            else:
                pass
```

测试结果样例，在输出结果是过滤了相似分低于0.3的词。在构造样本中有**“瓶治”**一次，应该和风险词**“平台”**相似，属于形近词，但该词打分为0.2，主要是在拆字部分，**将“治”拆为“水”和“台”，但将风险字“台”拆分为“厶 口”**，导致在字形部分无法相似，这部分的优化可在拆字前加一层判断，拆完一字后是否还需要拆另一字。

![截屏2022-06-19 下午5.22.17](/Users/didi/Library/Application Support/typora-user-images/截屏2022-06-19 下午5.22.17.png)

#### 后记

变体文本识别是个很有意思的方向，在文本纠错和黑灰产内容识别等场景应用广泛。本文中的部分脚本只是测试demo，效率较低，例如为了不漏过，使用的是滑动窗口取词比对，在风险字库和目标文本过长的情况下难以满足生产环境需求，在生产中可能需要关键词预提取等工作进行过滤；在打分过程中，可以结合专家经验或样本统计对特征加权重等方式让打分更符合实际场景需要。以上供参考